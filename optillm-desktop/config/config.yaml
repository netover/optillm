# Default settings for OptiLLM
llm_settings:
  model_name: "gpt-4-turbo"
  temperature: 0.7
  max_tokens: 2048
  stream_enabled: true

proxy_settings:
  port: 8080
  timeout_seconds: 120
  logging_enabled: false

features:
  caching_enabled: true
  safety_guard_enabled: false
